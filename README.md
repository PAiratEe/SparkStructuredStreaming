# ETL-коннвейер с использованием Spark Structured Streaming

Этот репозиторий содержит проект ETL-пайплайна для обработки потоковых данных с использованием **Apache Spark Structured Streaming**. Данные берутся из **Apache Kafka**, обрабатываются с помощью **Spark**, и результаты сохраняются в **Hadoop HDFS** для дальнейшего анализа.

## Содержание
- [Обзор](#обзор)
- [Архитектура](#архитектура)
- [Используемые технологии](#используемые-технологии)

---

## Обзор

Данный ETL-пайплайн предназначен для обработки потоковых данных в режиме реального времени. Компоненты пайплайна:
1. **Получение данных** – Spark получает данные из Kafka (два topic'а).
2. **Обработка данных** – Spark обрабатывает и трансформирует потоковые данные.
3. **Сохранение данных** – Обработанные данные сохраняются в HDFS для дальнейшего использования.

Пайплайн развертывается в среде **Kubernetes**, что обеспечивает его надежность и отказоустойчивость.

## Архитектура

Архитектура включает следующие компоненты:

- **Kubernetes** - оркестратор микросервисов.
  - **Spark** – выполняет обработку данных из Kafka в режиме Structured Streaming.
    - **Spark Driver** отвечает за отдачу команд Executor'ам.
    - **Spark Executor 1, 2** выполняет обработку данных и отправляет в HDFS.
  - **Monitoring Tools**
    - **Grafana** визуализирует ключевые метрики и оповещает о наличии каких-либо неполадок.
    - **Prometheus** забирает метрики из всех узлов и отправляет в Grafana.
  - **Hadoop** – распределенное хранилище для сохранения обработанных данных.

## Используемые технологии

- **Kubernetes** – для оркестрации контейнеров
- **Apache Spark** – для обработки больших данных
- **Hadoop HDFS** – для распределенного хранения обработанных данных
- **Prometheus + Grafana** - отвечает за наблюдаемость для конвейера, а также служит для оповещениях о сбоях
- **Docker** – для контейнеризации компонентов
